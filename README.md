
# **IoT Malware Detection â€” System Security Project**

Machine Learning Models + Scenario-Based Evaluation + Streamlit Dashboard

---

## **Project Overview**

This project builds an IoT intrusion-detection system using supervised machine learning.
We focus on **scenario-based evaluation**, where models are trained on a subset of CTU IoT captures and tested on entirely different real-world scenarios.

This prevents dataset leakage and measures whether the model can generalize to **new attacks**, **new devices**, and **new environments**.

The project includes:

* Decision Tree (baseline + tuned)
* Random Forest (baseline + tuned)
* Full preprocessing pipeline
* Scenario-based data split (realistic security evaluation)
* Streamlit dashboard for evaluation

---

## **Folder Structure**

```
SystemSecurityProject/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # ORIGINAL large CTU files (ignored in Git)
â”‚   â”œâ”€â”€ intermediate/       # Cleaned scenario CSVs used for training
â”‚   â””â”€â”€ processed/          # Scenario train/test splits (ignored in Git)
â”‚
â”œâ”€â”€ notebooks/              # Jupyter notebooks (EDA, debugging)
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ roc_curves/         # ROC curve PNGs
â”‚
â”œâ”€â”€ saved_models/           # .pkl models (Git-ignored)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â””â”€â”€ preprocess_scenario_split.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ decision_tree_scenario.py
â”‚   â”‚   â”œâ”€â”€ random_forest_scenario.py
â”‚   â”‚   â””â”€â”€ evaluate.py
â”‚   â””â”€â”€ dashboard/
â”‚       â””â”€â”€ app.py          # Streamlit dashboard
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

---

## **Installation & Setup**

### **1. Clone the repository**

```
git clone https://github.com/your-username/SystemSecurityProject.git
cd SystemSecurityProject
```

### **2. Create and activate virtual environment**

```
python3 -m venv .venv
source .venv/bin/activate  # macOS/Linux
```

### **3. Install dependencies**

```
pip install -r requirements.txt
```

### **4. Download intermediate CSV files**

Because raw CTU data is several GB, we provide **processed intermediate CSVs** via Google Drive:

 *Google Drive Link Here*

Download them into:

```
data/intermediate/
```

---

## ** Data Preprocessing (Scenario-Based)**

We use a **realistic scenario split** designed to mimic real intrusion detection conditions.

* **Train on scenarios 3-1, 8-1, 20-1, 21-1**
* **Test on unseen scenarios:** Somfy-01, 34-1, 42-1, 44-1, Honeypot 4-1, Honeypot 5-1

Run preprocessing:

```
python src/preprocessing/preprocess_scenario_split.py
```

This generates:

```
data/processed/X_train_scenario.csv
data/processed/X_test_scenario.csv
data/processed/y_train_scenario.csv
data/processed/y_test_scenario.csv
```

---

## **Models Included**

### **1. Decision Tree**

* Baseline
* Tuned via GridSearchCV
* Evaluates accuracy, precision, recall, F1, confusion matrix, ROC

Run:

```
python src/models/decision_tree_scenario.py
```

### **2. Random Forest**

* Baseline
* Tuned
* More robust generalization

Run:

```
python src/models/random_forest_scenario.py
```

### **3. Evaluation Utility**

Supports:

* Metrics
* ROC plots
* Confusion matrices

Run:

```
python src/models/evaluate.py
```

---

## **Streamlit Dashboard**

The dashboard visualizes:

* Model comparison
* Confusion matrices
* ROC curves
* Feature importance
* Threshold slider
* Optional CSV upload for user evaluation

Run the dashboard:

```
streamlit run src/dashboard/app.py
```

---

## **Whatâ€™s in `.gitignore`**

We ignore all large files:

```
# Raw CTU data (1â€“6 GB total)
data/raw/*

# Generated outputs
data/processed/*
saved_models/*

# Virtual environment
.venv/

# Python cache
__pycache__/
*.pyc
```

---

## ** Why Scenario-Based Splitting?**

Most student projects incorrectly:

* Merge all scenarios into one file
* Randomly split train/test
* Get 99â€“100% accuracy (data leakage)

This is **not realistic** and does **not** represent real IoT intrusion detection.

Our scenario split ensures:

* Training sees only certain devices and attacks
* Testing contains **entirely unseen** attacks, devices, and environments
* Results reflect **true generalization**

This is closer to real-world IDS deployment.

---

## **Summary of Findings**

| Model                    | Accuracy | Precision   | Recall | Notes                        |
| ------------------------ | -------- | ----------- | ------ | ---------------------------- |
| Decision Tree (baseline) | ~0.90    | Good        | Good   | Lightweight but unstable     |
| Decision Tree (tuned)    | ~0.90    | Same        | Same   | Depth limitedâ€”no improvement |
| Random Forest (baseline) | Higher   | More robust | Better | Handles variance better      |
| Random Forest (tuned)    | Best     | Best        | Best   | Recommended model            |

---

## **ğŸ‘¥ Team Responsibilities**

* Preprocessing: You
* Model training: You
* Dashboard: You
* Documentation: You
  (Modify this based on your team.)

---

## **Notes**

* Raw CTU logs are **NOT** included (size limitations)
* Use the Google Drive intermediate file set to run the project
* Project is reproducible end-to-end

---

If you want, I can also:

âœ” convert this README into a PDF
âœ” add badges (Python version, Streamlit, License, etc.)
âœ” add screenshots of dashboards or ROC curves
âœ” add equations for metrics

Just tell me **â€œadd visualsâ€** or **â€œPDF versionâ€**.
